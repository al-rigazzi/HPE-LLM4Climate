#!/usr/bin/env python3
"""
CPU-Optimized Real Llama Test with Zarr using conftest fixtures

This script tests with a smaller, CPU-friendly setup using conftest infrastructure:
- Uses conftest fixtures for model management
- Respects environment variables for configuration
- Processes smaller batches
- Uses shorter sequences
"""

import os
import sys
import time
from pathlib import Path

import numpy as np
import pytest
import torch
import torch.nn.functional as F

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

print("ğŸ–¥ï¸  CPU-Optimized Llama + AIFS + Zarr Test")
print("=" * 50)


@pytest.mark.integration
def test_lightweight_llama_zarr(aifs_llama_model):
    """Test with lightweight configuration for CPU using conftest fixtures."""

    try:
        from multimodal_aifs.utils.aifs_time_series_tokenizer import AIFSTimeSeriesTokenizer
        from multimodal_aifs.utils.zarr_data_loader import ZarrClimateLoader

        print("âœ… Modules imported")
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        pytest.fail(f"Import error: {e}")

    # Use model from conftest
    model = aifs_llama_model
    device = model.device
    print(f"ğŸ–¥ï¸  Device: {device}")
    print(f"âœ… Using model from conftest fixture")
    print(f"   ğŸ§  AIFS: {type(model.time_series_tokenizer).__name__}")
    print(f"   ğŸ¦™ LLM: {type(model.llama_model).__name__}")

    # Step 1: Load minimal climate data
    print(f"\nğŸ“Š Step 1: Loading Minimal Climate Data")
    print("-" * 40)

    try:
        loader = ZarrClimateLoader("test_climate.zarr")

        # Load just 2 timesteps for CPU efficiency
        climate_data = loader.load_time_range(
            "2024-01-01", "2024-01-01T03:00:00"  # Just 2 timesteps
        )

        # Convert to small tensor
        climate_tensor = loader.to_aifs_tensor(climate_data, batch_size=1, normalize=True)

        print(f"âœ… Climate tensor: {climate_tensor.shape}")
        print(f"   ğŸ’¾ Memory: {climate_tensor.numel() * 4 / 1e6:.1f} MB")

    except Exception as e:
        print(f"âŒ Failed to load climate data: {e}")
        pytest.fail(f"Failed to load climate data: {e}")

    # Step 2: Initialize lightweight Llama with heavy quantization
    print(f"\nğŸ¦™ Step 2: Initializing Quantized Llama")
    print("-" * 40)

    # Model already available from conftest fixture (respects environment variables)
    print("   âœ… Using model from conftest fixture (environment-controlled)")
    print(f"   ğŸ¯ Device: {device}")
    print(f"   ğŸ”§ Quantization: {os.environ.get('USE_QUANTIZATION', 'false')}")
    print(f"   ğŸ¦™ Mock LLM: {os.environ.get('USE_MOCK_LLM', 'false')}")

    # Step 3: Process with optimized settings
    print(f"\nğŸ”„ Step 3: CPU-Optimized Processing")
    print("-" * 40)

    try:
        # Tokenize climate data
        climate_tokens = model.tokenize_climate_data(climate_tensor)
        print(f"âœ… Climate tokens: {climate_tokens.shape}")

        # Simple text for CPU efficiency
        text_inputs = ["Analyze temperature data."]

        start_time = time.time()

        # Process with timeout protection
        with torch.no_grad():  # Disable gradients for inference
            result = model.process_climate_text(
                climate_tokens, text_inputs, task="embedding"  # Simpler task
            )

        elapsed = time.time() - start_time
        print(f"âœ… Processing complete in {elapsed:.1f}s")
        print(f"   ğŸ¯ Output shape: {result['fused_output'].shape}")

        if "generated_text" in result:
            print(f"   ğŸ’¬ Result: {result['generated_text']}")

    except Exception as e:
        print(f"âŒ Processing failed: {e}")
        pytest.fail(f"Processing failed: {e}")

    # Step 4: Memory efficiency check
    print(f"\nğŸ“Š Step 4: Memory Efficiency")
    print("-" * 40)

    try:
        # Count parameters
        total_params = sum(p.numel() for p in model.parameters())

        # Estimate memory usage
        param_memory_mb = total_params * 4 / 1e6  # Assume float32

        print(f"   ğŸ”¢ Total parameters: {total_params:,}")
        print(f"   ğŸ’¾ Parameter memory: {param_memory_mb:.1f} MB")

        # Check if quantization worked
        quantized_params = 0
        for param in model.parameters():
            if param.dtype in [torch.int8, torch.uint8]:
                quantized_params += param.numel()

        if quantized_params > 0:
            print(f"   âš—ï¸  Quantized parameters: {quantized_params:,}")
            print(f"   ğŸ“‰ Memory reduction: ~{100 * quantized_params / total_params:.1f}%")

    except Exception as e:
        print(f"âš ï¸  Memory check incomplete: {e}")

    print(f"\nğŸ‰ CPU Test Complete!")
    # Test passes by reaching this point without failures


@pytest.mark.integration
def test_compare_with_mock(aifs_llama_model):
    """Compare real vs mock performance using conftest fixtures."""
    print(f"\nâš–ï¸  Comparison: Real vs Mock Llama (conftest)")
    print("-" * 40)

    model = aifs_llama_model
    use_mock_env = os.environ.get("USE_MOCK_LLM", "").lower() in ("true", "1", "yes")

    print(f"   ğŸ§ª Testing current configuration:")
    print(f"   ğŸ”§ Using mock: {use_mock_env}")
    print(f"   ğŸ¯ Device: {model.device}")

    try:
        start_time = time.time()

        # Test the current model configuration
        param_count = sum(p.numel() for p in model.parameters())
        init_time = time.time() - start_time

        print(f"      âœ… Model ready in {init_time:.1f}s")
        print(f"      ğŸ”¢ Parameters: {param_count:,}")
        print(f"      ğŸ¯ Type: {'Mock' if use_mock_env else 'Real'} LLM")

        # Simple performance test
        dummy_data = torch.randn(1, 4, 3, 2, 2).to(model.device)
        dummy_text = ["Test query"]

        with torch.no_grad():
            test_start = time.time()
            result = model.forward(dummy_data, dummy_text, task="embedding")
            test_time = time.time() - test_start

        print(f"      âš¡ Inference time: {test_time:.3f}s")

    except Exception as e:
        print(f"      âŒ Failed: {e}")
        pytest.fail(f"Comparison test failed: {e}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
