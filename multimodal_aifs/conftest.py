"""
pytest Configuration and Fixtures for HPE-LLM4Climate

This file provides common fixtures and configuration for all tests in the project.
It includes fixtures for models, test data, and testing utilities.

Environment Variables:
- USE_MOCK_LLM: Set to "true" to force mock LLM usage instead of real models
- USE_QUANTIZATION: Set to "true" to enable quantization for real models
"""

import os
import sys
import types
import warnings
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock

import numpy as np
import pytest
import torch
from torch import nn

# Add project root to path
project_root = Path(__file__).parent.parent  # Go up one level to project root
sys.path.insert(0, str(project_root))


# =================== UTILITY FUNCTIONS ===================


def setup_flash_attn_mock():
    """Mock flash_attn to prevent import errors"""
    flash_attn_mock = types.ModuleType("flash_attn")
    # Don't set __spec__ as it causes type issues

    # Create flash_attn_interface submodule
    flash_attn_interface_mock = types.ModuleType("flash_attn_interface")
    flash_attn_interface_mock.flash_attn_func = MagicMock()  # type: ignore
    flash_attn_interface_mock.flash_attn_varlen_func = MagicMock()  # type: ignore

    # Set up the module hierarchy
    flash_attn_mock.flash_attn_interface = flash_attn_interface_mock  # type: ignore

    sys.modules["flash_attn"] = flash_attn_mock
    sys.modules["flash_attn.flash_attn_interface"] = flash_attn_interface_mock
    sys.modules["flash_attn_2_cuda"] = flash_attn_mock

    # Disable flash attention globally
    os.environ["USE_FLASH_ATTENTION"] = "false"
    os.environ["TRANSFORMERS_USE_FLASH_ATTENTION_2"] = "false"


def get_env_bool(env_var: str, default) -> bool:
    """Get boolean value from environment variable."""
    return os.environ.get(env_var, str(default)).lower() in ("true", "1", "yes")


def get_env_str(env_var: str, default: str) -> str:
    """Get string value from environment variable."""
    return os.environ.get(env_var, default)


# =================== PYTEST CONFIGURATION ===================


def pytest_configure(config):
    """Configure pytest with custom markers and settings."""
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "gpu: marks tests that require GPU (deselect with '-m \"not gpu\"')"
    )
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    config.addinivalue_line("markers", "unit: marks tests as unit tests")
    config.addinivalue_line("markers", "requires_llama: marks tests that require real Llama model")
    config.addinivalue_line("markers", "requires_aifs: marks tests that require real AIFS model")


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add markers automatically."""
    for item in items:
        # Auto-mark slow tests
        if "slow" in item.nodeid or "benchmark" in item.nodeid:
            item.add_marker(pytest.mark.slow)

        # Auto-mark GPU tests
        if "gpu" in item.nodeid or "cuda" in item.nodeid:
            item.add_marker(pytest.mark.gpu)

        # Auto-mark integration tests
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)

        # Auto-mark unit tests
        if "unit" in str(item.fspath):
            item.add_marker(pytest.mark.unit)


# =================== DEVICE AND ENVIRONMENT FIXTURES ===================


@pytest.fixture(scope="session")
def test_device():
    """Provide the best available device for testing."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


@pytest.fixture(scope="session")
def llm_mock_status():
    """Provide information about whether LLM mocking is enabled."""
    use_mock_llm = get_env_bool("USE_MOCK_LLM", True)
    use_quantization = get_env_bool("USE_QUANTIZATION", False)
    model_name = os.environ.get("LLM_MODEL_NAME", "meta-llama/Meta-Llama-3-8B")

    return {
        "use_mock_llm": use_mock_llm,
        "use_quantization": use_quantization,
        "model_name": model_name,
        "should_skip_real_llm_tests": use_mock_llm,
    }


@pytest.fixture(scope="session")
def zarr_dataset_path():
    """Get the zarr dataset path based on ZARR_SIZE environment variable."""
    zarr_size = get_env_str("ZARR_SIZE", "large").lower()

    # Map size to zarr file path
    size_to_path = {
        "tiny": "test_aifs_tiny.zarr",
        "small": "test_aifs_small.zarr",
        "large": "test_aifs_large.zarr",
    }

    if zarr_size not in size_to_path:
        raise ValueError(
            f"Invalid ZARR_SIZE '{zarr_size}'. Must be one of: {list(size_to_path.keys())}"
        )

    zarr_path = size_to_path[zarr_size]

    # Provide compatibility information
    print(f"üìÅ Using ZARR_SIZE='{zarr_size}' ‚Üí {zarr_path}")

    return zarr_path


@pytest.fixture(scope="session", autouse=True)
def ensure_test_zarr_dataset(zarr_dataset_path):  # pylint: disable=W0621
    """Ensure test Zarr dataset exists for integration tests."""
    # Path to the test zarr dataset - use AIFS-compatible format
    zarr_path = Path(zarr_dataset_path)

    # Check if dataset already exists
    if zarr_path.exists():
        print(f"‚úÖ Test Zarr dataset already exists: {zarr_path}")
        return str(zarr_path)

    print("üèóÔ∏è Creating test Zarr dataset for integration tests...")

    try:
        # Create a simple zarr dataset directly
        try:
            from datetime import datetime, timedelta

            import xarray as xr
        except ImportError as e:
            print(f"‚ùå Missing required packages for zarr creation: {e}")
            print("‚ö†Ô∏è Install with: pip install zarr xarray")
            return None

        # Create synthetic climate data in AIFS-compatible format
        time_steps = 2  # Match AIFS format
        n_variables = 10  # Reduced for tiny test dataset
        grid_points = 10000  # Reduced grid points for testing

        # Create coordinates matching AIFS format
        times = [datetime(2024, 1, 1) + timedelta(hours=i * 12) for i in range(time_steps)]
        variables = [
            "temperature_2m",
            "relative_humidity",
            "surface_pressure",
            "wind_speed_u",
            "wind_speed_v",
            "precipitation",
            "cloud_cover",
            "soil_moisture",
            "snow_depth",
            "radiation",
        ]

        # Create synthetic data with AIFS-compatible dimensions [batch, time, ensemble, grid, vars]
        # AIFS format: batch=1, time=2, ensemble=1, grid=542080, vars=10
        data = np.random.normal(0, 1, (1, time_steps, 1, grid_points, n_variables))

        # Create xarray dataset
        ds = xr.Dataset(
            {
                "data": xr.DataArray(
                    data,
                    dims=["batch", "time", "ensemble", "grid_points", "variables"],
                    coords={
                        "batch": [0],
                        "time": times,
                        "ensemble": [0],
                        "grid_points": range(grid_points),
                        "variables": variables,
                    },
                    attrs={"units": "normalized", "description": "Synthetic AIFS-compatible data"},
                )
            }
        )
        ds.attrs = {
            "title": "Synthetic AIFS-Compatible Dataset for Testing",
            "created": datetime.now().isoformat(),
            "description": "Small synthetic dataset in AIFS format "
            "[batch, time, ensemble, grid, vars]",
            "format": "AIFS-compatible",
        }

        # Save to zarr
        ds.to_zarr(zarr_path, mode="w")

        print(f"‚úÖ Test Zarr dataset created successfully: {zarr_path}")
        print(f"   Dimensions: [{time_steps}, {n_variables}, {grid_points}] (AIFS format)")

        return str(zarr_path)

    except Exception as e:
        print(f"‚ùå Failed to create test Zarr dataset: {e}")
        print(f"   Error type: {type(e).__name__}")
        # Don't fail the test session, just warn
        print("‚ö†Ô∏è Zarr tests may fail without test dataset")
        return None


# =================== LLM MODEL FIXTURES ===================


class MockLLMModel(nn.Module):
    """Mock LLM model for testing when real model is not available or requested."""

    def __init__(self, vocab_size: int = 32000, hidden_size: int = 4096):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.config = type(
            "Config", (), {"hidden_size": hidden_size, "vocab_size": vocab_size, "pad_token_id": 0}
        )()

        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_size, nhead=32, dim_feedforward=hidden_size * 4, batch_first=True
            ),
            num_layers=2,  # Reduced for testing
        )
        self.lm_head = nn.Linear(hidden_size, vocab_size)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: torch.Tensor | None = None, **kwargs
    ):
        """Forward pass mimicking LLM behavior."""
        x = self.embedding(input_ids)

        if attention_mask is not None:
            # Convert attention mask to transformer format
            src_key_padding_mask = ~attention_mask.bool()
        else:
            src_key_padding_mask = None

        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)
        logits = self.lm_head(x)

        # Return in HuggingFace-like format
        return type(
            "ModelOutput",
            (),
            {
                "logits": logits,
                "last_hidden_state": x,
                "hidden_states": (x,),
            },
        )()

    def generate(self, input_ids: torch.Tensor, max_length: int = 50, **kwargs):
        """Mock text generation."""
        current_length = input_ids.shape[1]

        # Simple mock generation - just repeat last token
        generated = input_ids.clone()

        for _ in range(min(max_length - current_length, 10)):  # Limit for testing
            with torch.no_grad():
                outputs = self.forward(generated)
                next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)
                generated = torch.cat([generated, next_token], dim=-1)

        return generated


@pytest.fixture(scope="session")
def llm_model_path():
    """Get path to local LLM model if available."""
    # Check for specific model name from environment
    model_name = os.environ.get("LLM_MODEL_NAME", "Meta-Llama-3-8B")

    possible_paths = [
        f"models/{model_name}",
        f"models/{model_name.lower()}",
        f"/models/{model_name}",
        os.path.expanduser(f"~/models/{model_name}"),
    ]

    for path in possible_paths:
        if Path(path).exists():
            return str(Path(path))

    return None


@pytest.fixture(scope="session")
def llm_model(llm_path, device):
    """
    Provide real LLM model or mock model based on USE_MOCK_LLM environment variable.
    """
    use_mock = get_env_bool("USE_MOCK_LLM", True)
    use_quantization = get_env_bool("USE_QUANTIZATION", False)
    model_name = os.environ.get("LLM_MODEL_NAME", "meta-llama/Meta-Llama-3-8B")

    print("ÔøΩ Loading LLM Model for Testing...")
    print(f"   Model: {model_name}")
    print(f"   Use Mock: {use_mock}")
    print(f"   Use Quantization: {use_quantization}")

    if use_mock:
        print("üé≠ Using mock LLM model (forced by USE_MOCK_LLM)")
        mock_model = MockLLMModel()
        mock_model.to(device)
        mock_model.eval()

        # Create a simple mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.encode.return_value = [1, 2, 3, 4, 5]
        mock_tokenizer.decode.return_value = "mock response"
        mock_tokenizer.pad_token_id = 0
        mock_tokenizer.eos_token_id = 2
        mock_tokenizer.vocab_size = 32000

        print(f"‚úÖ Mock LLM model created on {device}")
        return {
            "model": mock_model,
            "tokenizer": mock_tokenizer,
            "is_mock": True,
            "model_name": "MockLLM",
        }

    try:
        # Try to load real model
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # Setup flash attention mocking
        setup_flash_attn_mock()

        if llm_path is not None:
            print(f"üìÅ Found local model at: {llm_path}")
            model_path = llm_path
        else:
            print(f"üåê Using HuggingFace model: {model_name}")
            model_path = model_name

        print("üîÑ Loading real LLM model...")

        # Handle quantization
        quantization_config = None
        if use_quantization:
            try:
                from transformers import BitsAndBytesConfig

                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True
                )
                print("üîß Using 8-bit quantization")
            except ImportError:
                print("‚ö†Ô∏è Quantization requested but BitsAndBytesConfig not available")

        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, padding_side="left"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            device_map="auto" if device.type == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="eager",  # Disable flash attention
        )

        model.to(device)
        model.eval()

        print(f"‚úÖ Real LLM model loaded on {device}")
        return {
            "model": model,
            "tokenizer": tokenizer,
            "is_mock": False,
            "model_name": model_name,
        }

    except Exception as e:
        print(f"‚ö†Ô∏è Could not load real LLM model: {e}")
        print("üé≠ Falling back to mock LLM model...")

        mock_model = MockLLMModel()
        mock_model.to(device)
        mock_model.eval()

        # Create a simple mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.encode.return_value = [1, 2, 3, 4, 5]
        mock_tokenizer.decode.return_value = "mock response"
        mock_tokenizer.pad_token_id = 0
        mock_tokenizer.eos_token_id = 2
        mock_tokenizer.vocab_size = 32000

        print(f"‚úÖ Mock LLM model created on {device}")
        return {
            "model": mock_model,
            "tokenizer": mock_tokenizer,
            "is_mock": True,
            "model_name": "MockLLM",
        }


@pytest.fixture(scope="function")
def llm_tokenizer(model):
    """Provide the tokenizer from the llm_model fixture."""
    return model["tokenizer"]


# Legacy fixtures for backward compatibility
@pytest.fixture(scope="session")
def llama_model(model):
    """Legacy fixture - use llm_model instead."""
    warnings.warn("llama_model fixture is deprecated, use llm_model instead", DeprecationWarning)
    return model


@pytest.fixture(scope="function")
def llama_tokenizer(model):
    """Legacy fixture - use llm_tokenizer instead."""
    warnings.warn(
        "llama_tokenizer fixture is deprecated, use llm_tokenizer instead", DeprecationWarning
    )
    return model["tokenizer"]


# =================== AIFS MODEL FIXTURES ===================


@pytest.fixture(scope="session")
def aifs_model_available(test_device):  # pylint: disable=W0621
    """Check if AIFS model is available."""
    try:
        # Setup flash attention mocking before loading AIFS model
        setup_flash_attn_mock()

        from anemoi.inference.runners.simple import SimpleRunner

        # Try to initialize AIFS
        checkpoint = {"huggingface": "ecmwf/aifs-single-1.0"}
        runner = SimpleRunner(checkpoint, device=str(test_device))
        aifs_model_instance = runner.model.to(str(test_device))

        return True, runner, aifs_model_instance
    except Exception as e:
        print(f"‚ö†Ô∏è AIFS model not available: {e}")
        return False, None, None


@pytest.fixture(scope="session")
def aifs_model(aifs_model_available):  # pylint: disable=W0621
    """
    Provide real AIFS model if available, otherwise a mock.
    """
    print("üå™Ô∏è Loading AIFS Model for Testing...")

    available_flag, runner, model_instance = aifs_model_available

    if available_flag:
        print("‚úÖ Real AIFS model loaded")
        return {
            "runner": runner,
            "model": model_instance,
            "is_mock": False,
            "model_name": "AIFS-Single-1.0",
        }

    print("üé≠ Using mock AIFS model for testing...")

    # Create a simple mock AIFS model
    mock_runner = MagicMock()
    mock_model = MagicMock()

    # Mock the forward pass to return appropriate shapes
    def mock_forward(x):
        batch_size = x.shape[0] if hasattr(x, "shape") else 1
        return torch.randn(batch_size, 218)  # AIFS encoder output dimension

    # Use setattr to avoid mypy method assignment error
    setattr(mock_model, "forward", mock_forward)
    setattr(mock_model, "__call__", mock_forward)
    mock_runner.model = mock_model

    return {
        "runner": mock_runner,
        "model": mock_model,
        "is_mock": True,
        "model_name": "MockAIFS",
    }


# =================== AIFS + LLM FUSION MODEL FIXTURES ===================


class AIFSClimateTextFusionWrapper(nn.Module):
    """
    Wrapper around AIFSClimateTextFusion to provide the interface expected by tests.

    This wrapper adapts the production AIFSClimateTextFusion model to provide
    the same interface as the old AIFSLlamaFusionModel for backward compatibility.
    """

    def __init__(
        self,
        model,
        device_str: str = "cpu",
        fusion_dim: int = 512,
        use_mock_llama: bool = True,
        verbose: bool = False,
    ):
        super().__init__()
        self.device = device_str
        self.fusion_dim = fusion_dim

        # Add attributes expected by tests
        self.fusion_strategy = "cross_attention"
        self.time_series_dim = 218  # AIFS encoder produces 218 features

        # Initialize the real AIFSClimateTextFusion
        from multimodal_aifs.core.aifs_climate_fusion import AIFSClimateTextFusion

        self.fusion_model = AIFSClimateTextFusion(
            aifs_model=model,
            climate_dim=218,
            text_dim=768,
            fusion_dim=fusion_dim,
            device=device_str,
            verbose=verbose,
        )

        # Create a mock time series tokenizer for compatibility
        from multimodal_aifs.utils.aifs_time_series_tokenizer import AIFSTimeSeriesTokenizer

        self.time_series_tokenizer = AIFSTimeSeriesTokenizer(
            aifs_model=model,
            temporal_modeling="transformer",
            hidden_dim=256,  # Standard dimension
            device=device_str,
            verbose=verbose,
        )

        # Mock LLM attributes for compatibility
        self.llama_hidden_size = fusion_dim
        self.llama_tokenizer = None
        # Create a mock LLM model with parameters for testing compatibility
        self.llama_model = torch.nn.Linear(fusion_dim, fusion_dim)
        # Add vocab_size attribute for compatibility with tests
        setattr(self.llama_model, "vocab_size", 32000)  # Standard LLaMA vocab size
        self.use_mock_llama = use_mock_llama  # Respect the environment variable

    def tokenize_climate_data(self, climate_time_series: torch.Tensor) -> torch.Tensor:
        """
        Tokenize climate time series data using the AIFS tokenizer.

        Args:
            climate_time_series: [batch, time, vars, height, width]

        Returns:
            Time series tokens: [batch, time, time_series_dim]
        """
        return self.time_series_tokenizer.tokenize_time_series(climate_time_series)

    def tokenize_text(self, text_inputs: list) -> dict[str, torch.Tensor]:
        """
        Tokenize text inputs (mock implementation for compatibility).

        Args:
            text_inputs: List of text strings

        Returns:
            Dict with tokenized text (input_ids, attention_mask)
        """
        # Return mock tokens for testing compatibility
        batch_size = len(text_inputs)
        return {
            "input_ids": torch.randint(1, 1000, (batch_size, 32)).to(self.device),
            "attention_mask": torch.ones(batch_size, 32).to(self.device),
        }

    def process_climate_text(
        self, climate_tokens: torch.Tensor, text_inputs: list, task: str = "embedding"
    ) -> dict[str, Any]:
        """
        Process climate tokens and text inputs using the fusion model.

        Args:
            climate_tokens: Pre-computed climate tokens [batch, time, time_series_dim]
            text_inputs: List of text strings
            task: Task type ("generation", "embedding", "classification")

        Returns:
            Dict with task-specific outputs
        """
        # For now, create dummy 5D climate data since the fusion model expects it
        batch_size = climate_tokens.shape[0]
        dummy_climate_data = torch.randn(batch_size, 2, 1, 542080, 103).to(self.device)

        # Use the real fusion model
        try:
            result = self.fusion_model(dummy_climate_data, text_inputs)

            # Adapt the result format for compatibility
            adapted_result = {
                "fused_output": result["fused_features"],
                "climate_features": result["climate_features"],
                "text_features": result["text_features"],
            }

            # Add task-specific outputs
            if task == "generation":
                # For generation task, return logits (mock for now)
                adapted_result["logits"] = torch.randn(batch_size, 32, 32000).to(self.device)
                adapted_result["generated_text"] = (
                    f"Analysis of {text_inputs[0] if text_inputs else 'climate data'}: "
                    "The climate data shows interesting patterns."
                )
            elif task == "embedding":
                # For embedding task, return the fused features as embeddings
                adapted_result["embeddings"] = result["fused_features"]
                adapted_result["generated_text"] = "Embedding extraction completed successfully."
            elif task == "classification":
                # For classification task, return classification logits
                adapted_result["classification_logits"] = torch.randn(batch_size, 10).to(
                    self.device
                )
                adapted_result["generated_text"] = "Classification analysis completed."

            return adapted_result

        except Exception as e:
            print(f"‚ö†Ô∏è Fusion processing failed: {e}")
            # Return mock result for compatibility
            return {
                "fused_output": torch.randn(batch_size, 1, self.fusion_dim).to(self.device),
                "generated_text": "Mock analysis: Climate patterns processed successfully.",
            }

    def forward(self, climate_data, text_inputs, task="embedding"):
        """Forward pass through the fusion model."""
        return self.process_climate_text(
            self.tokenize_climate_data(climate_data), text_inputs, task
        )


@pytest.fixture(scope="module")
def aifs_llama_model(test_device, aifs_model):  # pylint: disable=W0621
    """
    Fixture to create AIFS + LLM fusion model.
    """
    # Setup flash attention mocking first
    setup_flash_attn_mock()

    # Get environment variables
    use_mock_llama = get_env_bool("USE_MOCK_LLM", True)
    # use_quantization and model_name are not used in this fixture

    print("üîó Creating AIFS+LLM Fusion Model...")
    print("   Using production AIFSClimateTextFusion model")

    # Use the actual AIFS model from the fixture
    actual_aifs_model = aifs_model["model"] if not aifs_model["is_mock"] else None

    if actual_aifs_model is None:
        print("   ‚ö†Ô∏è No real AIFS model available, using mock implementation")
        # Create a mock model for testing when AIFS is not available
        fusion_model = type(
            "MockFusionModel",
            (),
            {
                "time_series_tokenizer": None,
                "llama_hidden_size": 512,
                "llama_tokenizer": None,
                "llama_model": type(
                    "MockLLM", (), {"vocab_size": 32000}
                )(),  # Mock LLM with vocab_size
                "use_mock_llama": use_mock_llama,  # Respect the environment variable
                "tokenize_climate_data": lambda self, x: torch.randn(x.shape[0], 8, 256),
                "tokenize_text": lambda self, x: {
                    "input_ids": torch.randint(1, 1000, (len(x), 32)),
                    "attention_mask": torch.ones(len(x), 32),
                },
                "process_climate_text": lambda self, climate_tokens, _, task="embedding": {
                    "fused_output": torch.randn(climate_tokens.shape[0], 1, 512),
                    "generated_text": "Mock analysis completed.",
                },
                "forward": lambda self, climate_data, text_inputs, task="embedding": {
                    "fused_output": torch.randn(1, 1, 512),
                    "generated_text": "Mock analysis completed.",
                },
            },
        )()
    else:
        # Use the real production fusion model
        fusion_model = AIFSClimateTextFusionWrapper(
            model=actual_aifs_model,
            device_str=str(test_device),
            fusion_dim=512,
            use_mock_llama=use_mock_llama,  # Pass the environment variable value
            verbose=True,
        )

    print(f"‚úÖ AIFS+LLM Fusion Model created on {test_device}")
    return fusion_model


@pytest.fixture
def test_climate_data_fusion(test_device):  # pylint: disable=W0621
    """Fixture for test climate data specifically for fusion model testing"""

    # Create AIFS-compatible climate data: [batch, time, ensemble, grid, vars]
    # AIFS expects: batch=1, time=2, ensemble=1, grid=542080, vars=103
    climate_data = torch.randn(1, 2, 1, 542080, 103).to(test_device)
    text_inputs = ["Predict weather patterns based on the climate data."]

    return climate_data, text_inputs


# =================== TEST DATA FIXTURES ===================


@pytest.fixture(scope="session")
def test_climate_data(test_device):  # pylint: disable=W0621
    """Generate synthetic climate data for testing."""
    device = str(test_device)
    return {
        # 5D tensor for AIFS: [batch, time, ensemble, grid, vars]
        # batch=1, time=2 (AIFS expects exactly 2 timesteps: t-6h and t0),
        # ensemble=1, grid=542080 (real AIFS grid), vars=103
        "tensor_5d": torch.randn(1, 2, 1, 542080, 103).to(device),
        # 4D tensor: [batch, vars, height, width]
        "tensor_4d": torch.randn(2, 103, 32, 32).to(device),
        # Flattened for encoder: [batch, features]
        "tensor_2d": torch.randn(2, 218).to(device),
        # Variable names
        "variables": [
            "temperature_2m",
            "surface_pressure",
            "10m_u_component_of_wind",
            "10m_v_component_of_wind",
            "relative_humidity",
            "precipitation",
        ],
        # Coordinates
        "lat": np.linspace(-90, 90, 32),
        "lon": np.linspace(-180, 180, 32),
        "time": ["2024-01-01T00:00:00", "2024-01-01T06:00:00"],  # AIFS expects exactly 2 timesteps
    }


@pytest.fixture(scope="session")
def test_text_queries():
    """Provide sample text queries for testing."""
    return [
        "What is the temperature in New York?",
        "Show me precipitation patterns in California",
        "Analyze climate trends in Europe over the last decade",
        "How does El Ni√±o affect global weather patterns?",
        "Predict weather conditions for London next week",
        "Compare temperature anomalies between 2023 and 2024",
        "What are the climate implications of the monsoon season in India?",
        "Explain the relationship between sea surface temperature and hurricanes",
    ]


@pytest.fixture(scope="function")
def test_locations():
    """Provide test locations with coordinates."""
    return [
        {"name": "New York", "lat": 40.7128, "lon": -74.0060},
        {"name": "London", "lat": 51.5074, "lon": -0.1278},
        {"name": "Tokyo", "lat": 35.6762, "lon": 139.6503},
        {"name": "Sydney", "lat": -33.8688, "lon": 151.2093},
        {"name": "S√£o Paulo", "lat": -23.5505, "lon": -46.6333},
    ]


# =================== UTILITY FIXTURES ===================


@pytest.fixture(scope="function")
def temp_dir(tmp_path):
    """Provide a temporary directory for tests."""
    return tmp_path


@pytest.fixture(scope="session")
def get_project_root():
    """Provide the project root directory."""
    return Path(__file__).parent.parent


@pytest.fixture(scope="function")
def suppress_warnings():
    """Suppress common warnings during testing."""
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", UserWarning)
        warnings.simplefilter("ignore", FutureWarning)
        warnings.simplefilter("ignore", DeprecationWarning)
        yield


# =================== PERFORMANCE FIXTURES ===================


@pytest.fixture(scope="function")
def benchmark_config():
    """Configuration for performance benchmarks."""
    return {
        "batch_sizes": [1, 2, 4, 8],
        "sequence_lengths": [128, 256, 512],
        "num_iterations": 3,
        "warmup_iterations": 1,
    }


# =================== SKIP CONDITIONS ===================


def pytest_runtest_setup(item):
    """Setup function that runs before each test."""
    # Skip GPU tests if no GPU available
    if "gpu" in [mark.name for mark in item.iter_markers()]:
        if not torch.cuda.is_available():
            pytest.skip("GPU not available")

    # Skip tests requiring real models if they're not available
    if "requires_llama" in [mark.name for mark in item.iter_markers()]:
        # This will be checked by the llama_model fixture
        pass

    if "requires_aifs" in [mark.name for mark in item.iter_markers()]:
        # This will be checked by the aifs_model fixture
        pass


# =================== CLEANUP ===================


@pytest.fixture(scope="session", autouse=True)
def cleanup_session():
    """Clean up after test session."""
    yield

    # Clean up any CUDA memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("\nüßπ Test session cleanup completed")
