"""
pytest Configuration and Fixtures for HPE-LLM4Climate

Common fixtures and configuration for tests including models, data, and utilities.
Environment Variables: USE_MOCK_LLM, USE_QUANTIZATION
"""

import os
import sys
import types
import warnings
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock

import numpy as np
import pytest
import torch
from torch import nn

# Add project root to path
project_root = Path(__file__).parent.parent  # Go up one level to project root
sys.path.insert(0, str(project_root))

# Import AIFS constants
from multimodal_aifs.constants import (
    AIFS_GRID_POINTS,
    AIFS_INPUT_VARIABLES,
    AIFS_PROJECTED_ENCODER_OUTPUT_DIM,
    ALL_AIFS_VARIABLES,
)


# =================== UTILITY FUNCTIONS ===================
def setup_flash_attn_mock():
    """Mock flash_attn to prevent import errors - MacOS only"""
    import platform

    # Only mock flash attention on MacOS systems where it's incomplete
    if platform.system() != "Darwin":
        print("‚ÑπÔ∏è Skipping flash attention mock - not on MacOS")
        return
    flash_attn_mock = types.ModuleType("flash_attn")
    # Don't set __spec__ as it causes type issues
    # Create flash_attn_interface submodule
    flash_attn_interface_mock = types.ModuleType("flash_attn_interface")

    def mock_flash_attn_func(
        q,
        k,
        v,
        dropout_p=0.0,
        softmax_scale=None,
        causal=False,
        is_causal=None,
        return_attn_probs=False,
        **kwargs,
    ):
        """Mock flash attention function that returns proper tensor output - memory efficient."""
        # Handle both causal and is_causal parameter names
        if is_causal is not None:
            causal = is_causal

        # For very large sequences (like AIFS grid points), use a simplified approach
        # that avoids creating massive attention matrices
        seq_len = q.size(-2)

        # If sequence length is very large (> 10000), use a simplified identity-like operation
        # This is just for testing purposes and allows the model to run without OOM
        if seq_len > 10000:
            # Simple scaled passthrough that maintains dimensions
            # This is a fallback for testing - not a real attention implementation
            if softmax_scale is None:
                softmax_scale = 1.0 / (q.size(-1) ** 0.5)

            # Apply a simple scaling and return
            output = v * softmax_scale

            if return_attn_probs:
                # Return dummy attention weights for compatibility
                dummy_attn = torch.ones(
                    q.shape[:-1] + (k.shape[-2],), device=q.device, dtype=q.dtype
                )
                dummy_attn = dummy_attn / dummy_attn.sum(dim=-1, keepdim=True)
                return output, dummy_attn
            return output

        # For smaller sequences, use proper scaled dot-product attention
        if softmax_scale is None:
            softmax_scale = 1.0 / (q.size(-1) ** 0.5)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) * softmax_scale
        if causal:
            # Apply causal mask
            causal_mask = torch.triu(
                torch.ones(seq_len, seq_len, device=q.device, dtype=torch.bool), diagonal=1
            )
            scores = scores.masked_fill(causal_mask, float("-inf"))

        attn_weights = torch.softmax(scores, dim=-1)
        if dropout_p > 0.0 and q.training:
            attn_weights = torch.dropout(attn_weights, dropout_p, train=True)

        output = torch.matmul(attn_weights, v)

        if return_attn_probs:
            return output, attn_weights
        return output

    def mock_flash_attn_varlen_func(*args, **kwargs):
        """Mock variable length flash attention - simplified fallback."""
        # For variable length, just return the first argument (query) as a simple fallback
        if args:
            return args[0]  # Return query tensor
        return torch.zeros(1, 1, 1, 1)  # Fallback tensor

    setattr(flash_attn_interface_mock, "flash_attn_func", mock_flash_attn_func)
    setattr(flash_attn_interface_mock, "flash_attn_varlen_func", mock_flash_attn_varlen_func)
    # Set up the module hierarchy
    flash_attn_mock.flash_attn_interface = flash_attn_interface_mock  # type: ignore

    sys.modules["flash_attn"] = flash_attn_mock
    sys.modules["flash_attn.flash_attn_interface"] = flash_attn_interface_mock
    sys.modules["flash_attn_2_cuda"] = flash_attn_mock

    # Disable flash attention globally
    os.environ["USE_FLASH_ATTENTION"] = "false"
    os.environ["TRANSFORMERS_USE_FLASH_ATTENTION_2"] = "false"

    print("Flash attention mock enabled for MacOS")


def get_env_bool(env_var: str, default) -> bool:
    """Get boolean value from environment variable."""
    return os.environ.get(env_var, str(default)).lower() in ("true", "1", "yes")


def get_env_str(env_var: str, default: str) -> str:
    """Get string value from environment variable."""
    return os.environ.get(env_var, default)


# =================== PYTEST CONFIGURATION ===================
def pytest_sessionstart(session):
    """Set up global test environment at start of session."""
    # Suppress known MPS backend warnings for cleaner test output
    warnings.filterwarnings(
        "ignore",
        message=".*aten::scatter_reduce.two_out.*not currently supported on the MPS backend.*",
        category=UserWarning,
    )

    # Suppress common warnings that appear across multiple tests
    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=FutureWarning)
    warnings.filterwarnings("ignore", category=DeprecationWarning)

    # Set up default device for the entire test session
    if torch.cuda.is_available():
        default_device = torch.device("cuda")
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        default_device = torch.device("mps")
    else:
        default_device = torch.device("cpu")
    # Set the default device for PyTorch
    if hasattr(torch, "set_default_device"):
        torch.set_default_device(default_device)
    else:
        # Fallback for older PyTorch versions
        if default_device.type == "cuda":
            torch.cuda.set_device(default_device)
    print(f"Test session configured with default device: {default_device}")


def pytest_configure(config):
    """Configure pytest session with custom markers."""
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "gpu: marks tests that require GPU (deselect with '-m \"not gpu\"')"
    )
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    config.addinivalue_line("markers", "unit: marks tests as unit tests")
    config.addinivalue_line("markers", "requires_llama: marks tests that require real Llama model")
    config.addinivalue_line("markers", "requires_aifs: marks tests that require real AIFS model")
    config.addinivalue_line(
        "markers",
        "large_memory: marks tests that require high amounts of memory "
        "(deselect with '-m \"not large_memory\"')",
    )


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add markers automatically."""
    for item in items:
        # Auto-mark slow tests
        if "slow" in item.nodeid or "benchmark" in item.nodeid:
            item.add_marker(pytest.mark.slow)

        # Auto-mark GPU tests
        if "gpu" in item.nodeid or "cuda" in item.nodeid:
            item.add_marker(pytest.mark.gpu)

        # Auto-mark integration tests
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)

        # Auto-mark unit tests
        if "unit" in str(item.fspath):
            item.add_marker(pytest.mark.unit)


# =================== DEVICE AND ENVIRONMENT FIXTURES ===================
@pytest.fixture(scope="session")
def test_device():
    """Provide the best available device for testing."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


@pytest.fixture(scope="session")
def llm_mock_status():
    """Provide information about whether LLM mocking is enabled."""
    use_mock_llm = get_env_bool("USE_MOCK_LLM", False)
    use_quantization = get_env_bool("USE_QUANTIZATION", False)
    model_name = os.environ.get("LLM_MODEL_NAME", "meta-llama/Meta-Llama-3-8B")

    return {
        "use_mock_llm": use_mock_llm,
        "use_quantization": use_quantization,
        "model_name": model_name,
        "should_skip_real_llm_tests": use_mock_llm,
    }


@pytest.fixture(scope="session")
def zarr_dataset_path():
    """Get the zarr dataset path for testing."""
    # Use the standard test dataset path
    zarr_path = "test_aifs_large.zarr"

    print(f"Using test Zarr dataset: {zarr_path}")

    return zarr_path


@pytest.fixture(scope="session", autouse=True)
def ensure_test_zarr_dataset(zarr_dataset_path):  # pylint: disable=W0621
    """Ensure test Zarr dataset exists for integration tests."""
    # Path to the test zarr dataset - use AIFS-compatible format
    zarr_path = Path(zarr_dataset_path)
    # Check if dataset already exists
    if zarr_path.exists():
        print(f"Test Zarr dataset already exists: {zarr_path}")
        return str(zarr_path)
    print("üèóÔ∏è Creating test Zarr dataset for integration tests...")
    try:
        # Create a simple zarr dataset directly
        try:
            from datetime import datetime, timedelta

            import xarray as xr
        except ImportError as e:
            print(f"Missing required packages for zarr creation: {e}")
            print("Install with: pip install zarr xarray")
            return None
        # Create synthetic climate data in AIFS-compatible format
        # Use real AIFS dimensions as per copilot instructions
        time_steps = 2  # Match AIFS format
        grid_points = AIFS_GRID_POINTS  # Real AIFS grid points
        # Create coordinates matching AIFS format
        times = [datetime(2024, 1, 1) + timedelta(hours=i * 12) for i in range(time_steps)]
        variables = ALL_AIFS_VARIABLES  # Use the complete AIFS variable list from constants
        # Create synthetic data with AIFS-compatible dimensions [time, grid_point]
        # AIFS format: time=2, grid_point=10000
        data_shape = (time_steps, grid_points)

        # Create individual data variables for each climate variable
        data_vars = {}
        for var_name in variables:
            # Generate synthetic data for this variable
            var_data = np.random.normal(0, 1, data_shape)
            data_vars[var_name] = xr.DataArray(
                var_data,
                dims=["time", "grid_point"],
                coords={
                    "time": times,
                    "grid_point": range(grid_points),
                },
                attrs={"units": "normalized", "description": f"Synthetic {var_name} data"},
            )

        # Create xarray dataset with individual variables
        ds = xr.Dataset(data_vars)
        ds.attrs = {
            "title": "Synthetic AIFS-Compatible Dataset for Testing",
            "created": datetime.now().isoformat(),
            "description": (
                f"Synthetic dataset with {len(variables)} variables in AIFS format "
                "[time, grid_point]"
            ),
            "format": "AIFS-compatible",
            "aifs_grid_points": grid_points,
            "aifs_variables": len(variables),
            "aifs_timesteps": time_steps,
            "standard_aifs_dims": f"{time_steps}x{len(variables)}x{grid_points}",
            "note": "Data follows AIFS input format: [time, variables, grid_points]",
        }

        # Save to zarr
        ds.to_zarr(zarr_path, mode="w")

        print(f"Test Zarr dataset created successfully: {zarr_path}")
        print(f"   Dimensions: [{time_steps}, {len(variables)}, {grid_points}] (AIFS format)")

        return str(zarr_path)

    except Exception as e:
        print(f"Failed to create test Zarr dataset: {e}")
        print(f"   Error type: {type(e).__name__}")
        # Don't fail the test session, just warn
        print("Zarr tests may fail without test dataset")
        return None


# =================== LLM MODEL FIXTURES ===================
class MockLLMModel(nn.Module):
    """Mock LLM model for testing."""

    def __init__(self, vocab_size: int = 32000, hidden_size: int = 4096):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.config = type(
            "Config", (), {"hidden_size": hidden_size, "vocab_size": vocab_size, "pad_token_id": 0}
        )()

        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_size, nhead=32, dim_feedforward=hidden_size * 4, batch_first=True
            ),
            num_layers=2,  # Reduced for testing
        )
        self.lm_head = nn.Linear(hidden_size, vocab_size)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: torch.Tensor | None = None, **kwargs
    ):
        """Forward pass mimicking LLM behavior."""
        x = self.embedding(input_ids)

        if attention_mask is not None:
            # Convert attention mask to transformer format
            src_key_padding_mask = ~attention_mask.bool()
        else:
            src_key_padding_mask = None

        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)
        logits = self.lm_head(x)

        # Return in HuggingFace-like format
        return type(
            "ModelOutput",
            (),
            {
                "logits": logits,
                "last_hidden_state": x,
                "hidden_states": (x,),
            },
        )()

    def generate(self, input_ids: torch.Tensor, max_length: int = 50, **kwargs):
        """Mock text generation."""
        current_length = input_ids.shape[1]

        # Simple mock generation - just repeat last token
        generated = input_ids.clone()

        for _ in range(min(max_length - current_length, 10)):  # Limit for testing
            with torch.no_grad():
                outputs = self.forward(generated)
                next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)
                generated = torch.cat([generated, next_token], dim=-1)

        return generated


@pytest.fixture(scope="session")
def llm_model_path():
    """Get path to local LLM model if available."""
    # Check for specific model name from environment
    model_name = os.environ.get("LLM_MODEL_NAME", "Meta-Llama-3-8B")

    possible_paths = [
        f"models/{model_name}",
        f"models/{model_name.lower()}",
        f"/models/{model_name}",
        os.path.expanduser(f"~/models/{model_name}"),
    ]

    for path in possible_paths:
        if Path(path).exists():
            return str(Path(path))

    return None


@pytest.fixture(scope="session")
def llm_model(llm_path, device):
    """
    Provide real LLM model or mock model based on USE_MOCK_LLM environment variable.
    """
    if _MODEL_CACHE["llm_model"] is not None:
        print("‚ôªÔ∏è Reusing cached LLM model")
        return _MODEL_CACHE["llm_model"]

    use_mock = get_env_bool("USE_MOCK_LLM", True)
    use_quantization = get_env_bool("USE_QUANTIZATION", False)
    model_name = os.environ.get("LLM_MODEL_NAME", "meta-llama/Meta-Llama-3-8B")

    print("ü§ñ Loading LLM Model for Testing...")
    print(f"   Model: {model_name}")
    print(f"   Use Mock: {use_mock}")
    print(f"   Use Quantization: {use_quantization}")

    if use_mock:
        print("üé≠ Using mock LLM model (forced by USE_MOCK_LLM)")
        mock_model = MockLLMModel()
        mock_model.to(device)
        mock_model.eval()

        # Create a simple mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.encode.return_value = [1, 2, 3, 4, 5]
        mock_tokenizer.decode.return_value = "mock response"
        mock_tokenizer.pad_token_id = 0
        mock_tokenizer.eos_token_id = 2
        mock_tokenizer.vocab_size = 32000

        _MODEL_CACHE["llm_model"] = {
            "model": mock_model,
            "tokenizer": mock_tokenizer,
            "is_mock": True,
            "model_name": "MockLLM",
        }
        print(f"Mock LLM model created and cached on {device}")
        return _MODEL_CACHE["llm_model"]

    try:
        # Try to load real model
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # Setup flash attention mocking
        setup_flash_attn_mock()

        if llm_path is not None:
            print(f"Found local model at: {llm_path}")
            model_path = llm_path
        else:
            print(f"üåê Using HuggingFace model: {model_name}")
            model_path = model_name

        print("Loading real LLM model...")

        # Handle quantization
        quantization_config = None
        if use_quantization:
            try:
                from transformers import BitsAndBytesConfig

                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True
                )
                print("Using 8-bit quantization")
            except ImportError:
                print("Quantization requested but BitsAndBytesConfig not available")

        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, padding_side="left"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            torch_dtype=torch.float16 if device.type in ["cuda", "mps"] else torch.float32,
            device_map="auto" if device.type == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="eager",  # Disable flash attention
        )

        model.to(device)
        model.eval()

        _MODEL_CACHE["llm_model"] = {
            "model": model,
            "tokenizer": tokenizer,
            "is_mock": False,
            "model_name": model_name,
        }
        print(f"Real LLM model loaded and cached on {device}")
        return _MODEL_CACHE["llm_model"]

    except Exception as e:
        print(f"Could not load real LLM model: {e}")
        print("üé≠ Falling back to mock LLM model...")

        mock_model = MockLLMModel()
        mock_model.to(device)
        mock_model.eval()

        # Create a simple mock tokenizer
        mock_tokenizer = MagicMock()
        mock_tokenizer.encode.return_value = [1, 2, 3, 4, 5]
        mock_tokenizer.decode.return_value = "mock response"
        mock_tokenizer.pad_token_id = 0
        mock_tokenizer.eos_token_id = 2
        mock_tokenizer.vocab_size = 32000

        _MODEL_CACHE["llm_model"] = {
            "model": mock_model,
            "tokenizer": mock_tokenizer,
            "is_mock": True,
            "model_name": "MockLLM",
        }
        print(f"Mock LLM model created and cached on {device}")
        return _MODEL_CACHE["llm_model"]


@pytest.fixture(scope="function")
def llm_tokenizer(model):
    """Provide the tokenizer from the llm_model fixture."""
    return model["tokenizer"]


# =================== SINGLETON MODEL INSTANCES ===================

# Module-level cache to avoid re-instantiation of expensive models
# These are cached per pytest session to improve test performance
# Models are loaded once and reused across all tests in the session
_MODEL_CACHE: dict[str, Any] = {
    "aifs_model_available": None,  # Cached AIFS model availability check
    "aifs_model": None,  # Cached AIFS model instance
    "aifs_llama_model": None,  # Cached AIFS+LLM fusion model
    "llm_model": None,  # Cached LLM model instance
}


@pytest.fixture(scope="session")
def aifs_model_available(test_device):  # pylint: disable=W0621
    """Check if AIFS model is available."""
    if _MODEL_CACHE["aifs_model_available"] is not None:
        print("‚ôªÔ∏è Reusing cached AIFS model availability check")
        return _MODEL_CACHE["aifs_model_available"]

    print("Checking AIFS model availability...")

    # Check if we should force mock AIFS model
    use_mock_aifs = get_env_bool("USE_MOCK_AIFS", False)
    if use_mock_aifs:
        print("üé≠ Forcing mock AIFS model (USE_MOCK_AIFS=true)")
        _MODEL_CACHE["aifs_model_available"] = (False, None, None)
        return _MODEL_CACHE["aifs_model_available"]

    try:
        # Setup flash attention mocking before loading AIFS model
        setup_flash_attn_mock()

        from anemoi.inference.runners.simple import SimpleRunner

        # Try to initialize AIFS
        checkpoint = {"huggingface": "ecmwf/aifs-single-1.0"}
        runner = SimpleRunner(checkpoint, device=str(test_device))
        aifs_model_instance = runner.model.to(str(test_device))

        _MODEL_CACHE["aifs_model_available"] = (True, runner, aifs_model_instance)
        print("AIFS model availability cached")
        return _MODEL_CACHE["aifs_model_available"]
    except Exception as e:
        print(f"AIFS model not available: {e}")
        _MODEL_CACHE["aifs_model_available"] = (False, None, None)
        return _MODEL_CACHE["aifs_model_available"]


@pytest.fixture(scope="session")
def aifs_model(aifs_model_available):  # pylint: disable=W0621
    """
    Provide real AIFS model if available, otherwise a mock.
    """
    if _MODEL_CACHE["aifs_model"] is not None:
        print("‚ôªÔ∏è Reusing cached AIFS model")
        return _MODEL_CACHE["aifs_model"]

    print("Loading AIFS Model for Testing...")

    available_flag, runner, model_instance = aifs_model_available

    if available_flag:
        print("Real AIFS model loaded and cached")
        _MODEL_CACHE["aifs_model"] = {
            "runner": runner,
            "model": model_instance,
            "is_mock": False,
            "model_name": "AIFS-Single-1.0",
        }
        return _MODEL_CACHE["aifs_model"]

    print("üé≠ Using mock AIFS model for testing...")

    # Create a simple mock AIFS model
    mock_runner = MagicMock()
    mock_model = MagicMock()

    # Mock the forward pass to return appropriate shapes
    def mock_forward(x):
        batch_size = x.shape[0] if hasattr(x, "shape") else 1
        # AIFS encoder output dimension
        return torch.randn(batch_size, AIFS_PROJECTED_ENCODER_OUTPUT_DIM)

    # Use setattr to avoid mypy method assignment error
    setattr(mock_model, "forward", mock_forward)
    setattr(mock_model, "__call__", mock_forward)
    mock_runner.model = mock_model

    _MODEL_CACHE["aifs_model"] = {
        "runner": mock_runner,
        "model": mock_model,
        "is_mock": True,
        "model_name": "MockAIFS",
    }
    print("Mock AIFS model cached")
    return _MODEL_CACHE["aifs_model"]


# =================== AIFS + LLM FUSION MODEL FIXTURES ===================
class AIFSClimateTextFusionWrapper(nn.Module):
    """
    Wrapper around AIFSClimateTextFusion for test compatibility.
    """

    def __init__(
        self,
        model,
        device_str: str = "cpu",
        fusion_dim: int = 512,
        use_mock_llama: bool = True,
        verbose: bool = False,
    ):
        super().__init__()
        self.device = device_str
        self.fusion_dim = fusion_dim

        # Add attributes expected by tests
        self.fusion_strategy = "cross_attention"
        self.time_series_dim = AIFS_PROJECTED_ENCODER_OUTPUT_DIM  # AIFS encoder produces features

        # Initialize the real AIFSClimateTextFusion
        from multimodal_aifs.core.aifs_climate_fusion import AIFSClimateTextFusion

        self.fusion_model = AIFSClimateTextFusion(
            aifs_model=model,
            climate_dim=AIFS_PROJECTED_ENCODER_OUTPUT_DIM,
            text_dim=768,
            fusion_dim=fusion_dim,
            device=device_str,
            dtype=torch.float16 if device_str in ["cuda", "mps"] else torch.float32,
            verbose=verbose,
        )

        # Create a mock time series tokenizer for compatibility
        from multimodal_aifs.utils.aifs_time_series_tokenizer import AIFSTimeSeriesTokenizer

        self.time_series_tokenizer = AIFSTimeSeriesTokenizer(
            aifs_model=model,
            temporal_modeling="transformer",
            hidden_dim=256,  # Standard dimension
            device=device_str,
            dtype=torch.float16 if device_str in ["cuda", "mps"] else torch.float32,
            verbose=verbose,
        )

        # Mock LLM attributes for compatibility
        self.llama_hidden_size = fusion_dim
        self.llama_tokenizer = None
        # Create a mock LLM model with parameters for testing compatibility
        self.llama_model = torch.nn.Linear(
            fusion_dim,
            fusion_dim,
            dtype=torch.float16 if device_str in ["cuda", "mps"] else torch.float32,
        )
        # Add vocab_size attribute for compatibility with tests
        setattr(self.llama_model, "vocab_size", 32000)  # Standard LLaMA vocab size
        self.use_mock_llama = use_mock_llama  # Respect the environment variable

    def tokenize_climate_data(self, climate_time_series: torch.Tensor) -> torch.Tensor:
        """
        Tokenize climate time series data using the AIFS tokenizer.

        Args:
            climate_time_series: [batch, time, vars, height, width]

        Returns:
            Time series tokens: [batch, time, time_series_dim]
        """
        return self.time_series_tokenizer.tokenize_time_series(climate_time_series)

    def tokenize_text(self, text_inputs: list) -> dict[str, torch.Tensor]:
        """
        Tokenize text inputs (mock implementation).
        Returns dict with input_ids and attention_mask.
        """
        # Return mock tokens for testing compatibility
        batch_size = len(text_inputs)
        return {
            "input_ids": torch.randint(1, 1000, (batch_size, 32)).to(self.device),
            "attention_mask": torch.ones(batch_size, 32).to(self.device),
        }

    def process_climate_text(
        self, climate_tokens: torch.Tensor, text_inputs: list, task: str = "embedding"
    ) -> dict[str, Any]:
        """Process climate tokens and text inputs using fusion model."""
        # For now, create dummy 5D climate data since the fusion model expects it
        batch_size = climate_tokens.shape[0]
        dummy_climate_data = torch.randn(
            batch_size, 2, 1, AIFS_GRID_POINTS, AIFS_INPUT_VARIABLES
        ).to(self.device)

        # Use the real fusion model
        try:
            result = self.fusion_model(dummy_climate_data, text_inputs)

            # Adapt the result format for compatibility
            adapted_result = {
                "fused_output": result["fused_features"],
                "climate_features": result["climate_features"],
                "text_features": result["text_features"],
            }

            # Add task-specific outputs
            if task == "generation":
                # For generation task, return logits (mock for now)
                adapted_result["logits"] = torch.randn(batch_size, 32, 32000).to(self.device)
                adapted_result["generated_text"] = (
                    f"Analysis of {text_inputs[0] if text_inputs else 'climate data'}: "
                    "The climate data shows interesting patterns."
                )
            elif task == "embedding":
                # For embedding task, return the fused features as embeddings
                adapted_result["embeddings"] = result["fused_features"]
                adapted_result["generated_text"] = "Embedding extraction completed successfully."
            elif task == "classification":
                # For classification task, return classification logits
                adapted_result["classification_logits"] = torch.randn(batch_size, 10).to(
                    self.device
                )
                adapted_result["generated_text"] = "Classification analysis completed."

            return adapted_result

        except Exception as e:
            print(f"Fusion processing failed: {e}")
            # Return mock result for compatibility
            return {
                "fused_output": torch.randn(batch_size, 1, self.fusion_dim).to(self.device),
                "generated_text": "Mock analysis: Climate patterns processed successfully.",
            }

    def forward(self, climate_data, text_inputs, task="embedding"):
        """Forward pass through the fusion model."""
        return self.process_climate_text(
            self.tokenize_climate_data(climate_data), text_inputs, task
        )


@pytest.fixture(scope="module")
def aifs_llama_model(test_device, aifs_model):  # pylint: disable=W0621
    """
    Fixture to create AIFS + LLM fusion model.
    """
    if _MODEL_CACHE["aifs_llama_model"] is not None:
        print("‚ôªÔ∏è Reusing cached AIFS+LLM fusion model")
        return _MODEL_CACHE["aifs_llama_model"]

    # Setup flash attention mocking first
    setup_flash_attn_mock()

    # Get environment variables
    use_mock_llama = get_env_bool("USE_MOCK_LLM", True)
    # use_quantization and model_name are not used in this fixture

    print("üîó Creating AIFS+LLM Fusion Model...")
    print("   Using production AIFSClimateTextFusion model")

    # Use the actual AIFS model from the fixture
    actual_aifs_model = aifs_model["model"] if not aifs_model["is_mock"] else None

    if actual_aifs_model is None:
        print("   No real AIFS model available, using mock implementation")
        # Create a mock model for testing when AIFS is not available
        fusion_model = type(
            "MockFusionModel",
            (),
            {
                "device": str(test_device),  # Add device attribute
                "time_series_tokenizer": None,
                "llama_hidden_size": 512,
                "llama_tokenizer": None,
                "llama_model": type(
                    "MockLLM", (), {"vocab_size": 32000}
                )(),  # Mock LLM with vocab_size
                "use_mock_llama": use_mock_llama,  # Respect the environment variable
                "tokenize_climate_data": lambda self, x: torch.randn(x.shape[0], 8, 256),
                "tokenize_text": lambda self, x: {
                    "input_ids": torch.randint(1, 1000, (len(x), 32)),
                    "attention_mask": torch.ones(len(x), 32),
                },
                "process_climate_text": lambda self, climate_tokens, _, task="embedding": {
                    "fused_output": torch.randn(climate_tokens.shape[0], 1, 512),
                    "generated_text": "Mock analysis completed.",
                },
                "forward": lambda self, climate_data, text_inputs, task="embedding": {
                    "fused_output": torch.randn(1, 1, 512),
                    "generated_text": "Mock analysis completed.",
                },
                "parameters": lambda self: iter(
                    [
                        torch.randn(512, 256),  # Mock parameter tensor 1
                        torch.randn(512),  # Mock parameter tensor 2 (bias)
                        torch.randn(256, 128),  # Mock parameter tensor 3
                    ]
                ),
            },
        )()
    else:
        # Use the real production fusion model
        fusion_model = AIFSClimateTextFusionWrapper(
            model=actual_aifs_model,
            device_str=str(test_device),
            fusion_dim=512,
            use_mock_llama=use_mock_llama,  # Pass the environment variable value
            verbose=True,
        )

    _MODEL_CACHE["aifs_llama_model"] = fusion_model
    print(f"AIFS+LLM Fusion Model created and cached on {test_device}")
    return fusion_model


@pytest.fixture
def test_climate_data_fusion(test_device):  # pylint: disable=W0621
    """Fixture for test climate data for fusion model testing"""
    # Create AIFS-compatible climate data: [batch, time, ensemble, grid, vars]
    # AIFS expects: batch=1, time=2, ensemble=1, grid=AIFS_GRID_POINTS, vars=AIFS_INPUT_VARIABLES
    climate_data = torch.randn(1, 2, 1, AIFS_GRID_POINTS, AIFS_INPUT_VARIABLES).to(test_device)
    text_inputs = ["Predict weather patterns based on the climate data."]
    return climate_data, text_inputs


# =================== TEST DATA FIXTURES ===================
@pytest.fixture(scope="session")
def test_climate_data(test_device):  # pylint: disable=W0621
    """Generate synthetic climate data for testing."""
    device = str(test_device)

    # Determine the appropriate floating point format based on device
    if test_device.type in ["cuda", "mps"]:
        dtype = torch.float16
    else:
        dtype = torch.float32

    return {
        # 5D tensor for AIFS: [batch, time, ensemble, grid, vars]
        # batch=1, time=2 (AIFS expects exactly 2 timesteps: t-6h and t0),
        # ensemble=1, grid=AIFS_GRID_POINTS (real AIFS grid), vars=AIFS_INPUT_VARIABLES
        "tensor_5d": torch.randn(1, 2, 1, AIFS_GRID_POINTS, AIFS_INPUT_VARIABLES, dtype=dtype).to(
            device
        ),
        # 4D tensor: [batch, vars, height, width]
        "tensor_4d": torch.randn(2, AIFS_INPUT_VARIABLES, 32, 32, dtype=dtype).to(device),
        # Flattened for encoder: [batch, features]
        "tensor_2d": torch.randn(2, AIFS_PROJECTED_ENCODER_OUTPUT_DIM, dtype=dtype).to(device),
        # Variable names
        "variables": [
            "temperature_2m",
            "surface_pressure",
            "10m_u_component_of_wind",
            "10m_v_component_of_wind",
            "relative_humidity",
            "precipitation",
        ],
        # Coordinates
        "lat": np.linspace(-90, 90, 32),
        "lon": np.linspace(-180, 180, 32),
        "time": ["2024-01-01T00:00:00", "2024-01-01T06:00:00"],  # AIFS expects exactly 2 timesteps
    }


@pytest.fixture(scope="session")
def test_text_queries():
    """Provide sample text queries for testing."""
    return [
        "What is the temperature in New York?",
        "Show me precipitation patterns in California",
        "Analyze climate trends in Europe over the last decade",
        "How does El Ni√±o affect global weather patterns?",
        "Predict weather conditions for London next week",
        "Compare temperature anomalies between 2023 and 2024",
        "What are the climate implications of the monsoon season in India?",
        "Explain the relationship between sea surface temperature and hurricanes",
    ]


@pytest.fixture(scope="function")
def test_locations():
    """Provide test locations with coordinates."""
    return [
        {"name": "New York", "lat": 40.7128, "lon": -74.0060},
        {"name": "London", "lat": 51.5074, "lon": -0.1278},
        {"name": "Tokyo", "lat": 35.6762, "lon": 139.6503},
        {"name": "Sydney", "lat": -33.8688, "lon": 151.2093},
        {"name": "S√£o Paulo", "lat": -23.5505, "lon": -46.6333},
    ]


# =================== UTILITY FIXTURES ===================
@pytest.fixture(scope="function")
def temp_dir(tmp_path):
    """Provide a temporary directory for tests."""
    return tmp_path


@pytest.fixture(scope="session")
def get_project_root():
    """Provide the project root directory."""
    return Path(__file__).parent.parent


@pytest.fixture(scope="function")
def suppress_warnings():
    """Suppress common warnings during testing."""
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", UserWarning)
        warnings.simplefilter("ignore", FutureWarning)
        warnings.simplefilter("ignore", DeprecationWarning)
        yield


# =================== PERFORMANCE FIXTURES ===================
@pytest.fixture(scope="function")
def benchmark_config():
    """Configuration for performance benchmarks."""
    return {
        "batch_sizes": [1, 2, 4, 8],
        "sequence_lengths": [128, 256, 512],
        "num_iterations": 3,
        "warmup_iterations": 1,
    }


# =================== SKIP CONDITIONS ===================
def pytest_runtest_setup(item):
    """Setup function that runs before each test."""
    # Skip GPU tests if no GPU available
    if "gpu" in [mark.name for mark in item.iter_markers()]:
        if not torch.cuda.is_available():
            pytest.skip("GPU not available")


# =================== CLEANUP ===================
@pytest.fixture(scope="session", autouse=True)
def cleanup_session():
    """Clean up after test session."""
    yield

    # Clean up any CUDA memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("\nüßπ Test session cleanup completed")
