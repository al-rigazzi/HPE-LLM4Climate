name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggering

env:
  # Environment variables for testing
  PYTHONPATH: ${{ github.workspace }}
  HF_HUB_DISABLE_SYMLINKS_WARNING: 1
  TOKENIZERS_PARALLELISM: false

# Cancel previous runs when new push/PR is made
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        os: [ubuntu-24.04, macos-latest]
        python-version: ['3.12', '3.13']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-24.04'
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          libhdf5-dev \
          libnetcdf-dev \
          libgeos-dev \
          libproj-dev \
          libgdal-dev \
          gdal-bin

    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        brew update
        brew install hdf5 netcdf geos proj gdal

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip wheel setuptools
        pip install -r requirements.txt

    - name: Install additional test dependencies
      run: |
        pip install pytest pytest-xvfb pytest-cov pytest-timeout
        pip install geopy nominatim

    - name: Create test data directories
      run: |
        mkdir -p data/weights
        mkdir -p data/climatology

    - name: Create test weights script
      run: |
        cat > create_test_weights.py << 'EOF'
        import torch
        import os

        # Create mock Prithvi encoder weights for testing
        config = {
            'params': {
                'in_channels': 160,
                'input_size_time': 2,
                'in_channels_static': 11,
                'embed_dim': 2560,
                'n_blocks_encoder': 25,
                'n_heads': 16,
                'mlp_multiplier': 4,
                'dropout': 0.0,
                'patch_size_px': [2, 2]
            }
        }

        # Create minimal state dict for testing
        state_dict = {
            'patch_embedding.proj.weight': torch.randn(2560, 320, 2, 2),
            'patch_embedding.proj.bias': torch.randn(2560),
            'patch_embedding_static.proj.weight': torch.randn(2560, 168, 2, 2),
            'patch_embedding_static.proj.bias': torch.randn(2560),
            'input_scalers_mu': torch.zeros(1, 1, 160, 1, 1),
            'input_scalers_sigma': torch.ones(1, 1, 160, 1, 1),
            'static_input_scalers_mu': torch.zeros(1, 11, 1, 1),
            'static_input_scalers_sigma': torch.ones(1, 11, 1, 1),
            'mask_token': torch.zeros(1, 1, 1, 2560),
            'input_time_embedding.weight': torch.randn(640, 1),
            'input_time_embedding.bias': torch.randn(640)
        }

        # Add a few transformer blocks for testing
        for i in range(3):  # Just a few blocks for CI
            state_dict.update({
                f'encoder.lgl_block.transformers.{i}.attention.0.weight': torch.randn(2560),
                f'encoder.lgl_block.transformers.{i}.attention.0.bias': torch.randn(2560),
                f'encoder.lgl_block.transformers.{i}.attention.1.qkv_layer.weight': torch.randn(7680, 2560),
                f'encoder.lgl_block.transformers.{i}.attention.1.w_layer.weight': torch.randn(2560, 2560),
                f'encoder.lgl_block.transformers.{i}.ff.0.weight': torch.randn(2560),
                f'encoder.lgl_block.transformers.{i}.ff.0.bias': torch.randn(2560),
                f'encoder.lgl_block.transformers.{i}.ff.1.net.0.weight': torch.randn(10240, 2560),
                f'encoder.lgl_block.transformers.{i}.ff.1.net.0.bias': torch.randn(10240),
                f'encoder.lgl_block.transformers.{i}.ff.1.net.3.weight': torch.randn(2560, 10240),
                f'encoder.lgl_block.transformers.{i}.ff.1.net.3.bias': torch.randn(2560)
            })

        checkpoint = {
            'model_state_dict': state_dict,
            'config': config
        }

        # Save test encoder files
        os.makedirs('data/weights', exist_ok=True)
        torch.save(checkpoint, 'data/weights/prithvi_encoder.pt')
        torch.save(checkpoint, 'data/weights/prithvi_encoder_fixed.pt')

        print('✅ Created mock encoder weights for CI testing')
        EOF

    - name: Create mock weight files for testing
      run: |
        python create_test_weights.py

    - name: Create verification script
      run: |
        cat > verify_installation.py << 'EOF'
        import sys
        print(f'Python version: {sys.version}')
        print(f'Platform: {sys.platform}')

        # Test core imports
        try:
            import torch
            print(f'✅ PyTorch {torch.__version__}')

            # Test MPS availability on macOS
            if sys.platform == 'darwin' and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                print('✅ MPS available')
            else:
                print('ℹ️  MPS not available (expected on Linux)')

        except ImportError as e:
            print(f'❌ PyTorch import failed: {e}')
            sys.exit(1)

        try:
            import transformers
            print(f'✅ Transformers {transformers.__version__}')
        except ImportError:
            print('⚠️  Transformers not available')

        try:
            import geopy
            print(f'✅ GeoPy {geopy.__version__}')
        except ImportError:
            print('⚠️  GeoPy not available')
        EOF

    - name: Verify installation
      run: |
        python verify_installation.py

    - name: Run location-aware tests
      timeout-minutes: 10
      run: |
        python -m pytest multimodal/tests/test_location_aware.py -v --tb=short --timeout=300

    - name: Run fusion tests
      timeout-minutes: 10
      run: |
        python -m pytest multimodal/tests/test_fusion.py -v --tb=short --timeout=300

    - name: Run encoder extractor tests
      timeout-minutes: 10
      run: |
        python -m pytest multimodal/tests/test_encoder_extractor.py -v --tb=short --timeout=300

    - name: Run comprehensive test suite
      timeout-minutes: 15
      run: |
        echo "🧪 Running comprehensive test suite..."
        echo "==========================================="

        echo "1. Location-Aware Tests:"
        python multimodal/tests/test_location_aware.py

        echo -e "\n2. Fusion Tests:"
        python multimodal/tests/test_fusion.py

        echo -e "\n3. Encoder Extractor Tests:"
        python multimodal/tests/test_encoder_extractor.py

        echo -e "\n✅ All tests completed successfully!"

    - name: Create import test script
      run: |
        cat > test_imports.py << 'EOF'
        from multimodal.core.location_aware import GeographicResolver, SpatialCropper, LocationAwareAttention
        from multimodal.core.climate_text_fusion import ClimateTextFusion
        from multimodal.core.location_aware_fusion import LocationAwareClimateAnalysis
        from multimodal.utils.encoder_extractor import PrithviWxC_Encoder, extract_encoder_weights

        print('✅ All multimodal imports successful')

        # Test basic functionality
        resolver = GeographicResolver()
        result = resolver.resolve_location('Stockholm, Sweden')
        print(f'✅ Geographic resolution working: {result.name if result else "No result"}')

        # Test encoder creation (without loading heavy weights)
        print('✅ Core system components verified')
        EOF

    - name: Test multimodal imports
      run: |
        python test_imports.py

  lint:
    name: Code Quality
    runs-on: ubuntu-24.04

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install linting dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy

    - name: Check code formatting with Black
      run: |
        black --check --diff multimodal/ || echo "⚠️  Black formatting issues found (non-blocking)"

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff multimodal/ || echo "⚠️  Import sorting issues found (non-blocking)"

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 multimodal/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Treat other issues as warnings
        flake8 multimodal/ --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics || echo "⚠️  Linting warnings found (non-blocking)"

  integration:
    name: Integration Tests
    runs-on: ubuntu-24.04
    needs: [test]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install geopy nominatim

    - name: Create integration test data
      run: |
        mkdir -p data/weights
        cat > create_integration_weights.py << 'EOF'
        import torch

        # Create minimal test weights
        config = {'params': {'in_channels': 160, 'in_channels_static': 11, 'embed_dim': 2560, 'n_blocks_encoder': 25}}
        state_dict = {
            'patch_embedding.proj.weight': torch.randn(2560, 320, 2, 2),
            'patch_embedding.proj.bias': torch.randn(2560),
            'patch_embedding_static.proj.weight': torch.randn(2560, 168, 2, 2),
            'patch_embedding_static.proj.bias': torch.randn(2560),
            'input_scalers_mu': torch.zeros(1, 1, 160, 1, 1),
            'input_scalers_sigma': torch.ones(1, 1, 160, 1, 1),
            'static_input_scalers_mu': torch.zeros(1, 11, 1, 1),
            'static_input_scalers_sigma': torch.ones(1, 11, 1, 1),
            'mask_token': torch.zeros(1, 1, 1, 2560)
        }
        torch.save({'model_state_dict': state_dict, 'config': config}, 'data/weights/prithvi_encoder.pt')
        EOF
        python create_integration_weights.py

    - name: Create end-to-end test script
      run: |
        cat > test_end_to_end.py << 'EOF'
        import torch
        import warnings
        warnings.filterwarnings('ignore')

        print('🧪 Testing end-to-end workflow...')

        # Test core imports
        from multimodal.core.location_aware import GeographicResolver
        from multimodal.utils.encoder_extractor import PrithviWxC_Encoder

        # Test geographic resolution
        resolver = GeographicResolver()
        result = resolver.resolve_location('Paris, France')
        print(f'✅ Geographic resolution: {result.name if result else "Failed"}')

        # Test encoder loading
        try:
            encoder_data = torch.load('data/weights/prithvi_encoder.pt', map_location='cpu')
            print(f'✅ Encoder loading: {len(encoder_data["model_state_dict"])} weights')
        except Exception as e:
            print(f'❌ Encoder loading failed: {e}')

        print('✅ End-to-end workflow test completed')
        EOF

    - name: Test end-to-end workflow
      timeout-minutes: 5
      run: |
        python test_end_to_end.py

  security:
    name: Security Scan
    runs-on: ubuntu-24.04

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit

    - name: Check for known vulnerabilities
      run: |
        safety check --json || echo "⚠️  Security vulnerabilities found (non-blocking)"

    - name: Run bandit security scan
      run: |
        bandit -r multimodal/ -f json || echo "⚠️  Security issues found (non-blocking)"

  docs:
    name: Documentation Check
    runs-on: ubuntu-24.04

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check README and documentation
      run: |
        echo "📚 Checking documentation..."

        # Check that key files exist
        test -f README.md || { echo "❌ Missing README.md"; exit 1; }
        test -f multimodal/README.md || { echo "❌ Missing multimodal/README.md"; exit 1; }
        test -f requirements.txt || { echo "❌ Missing requirements.txt"; exit 1; }

        # Check for Python docstrings in core files
        grep -l '"""' multimodal/core/*.py || echo "⚠️  Some files missing docstrings"

        echo "✅ Documentation check completed"

  summary:
    name: CI Summary
    runs-on: ubuntu-24.04
    needs: [test, lint, integration, security, docs]
    if: always()

    steps:
    - name: Summary
      run: |
        echo "🎯 CI/CD Pipeline Summary"
        echo "========================"
        echo "Test Status: ${{ needs.test.result }}"
        echo "Lint Status: ${{ needs.lint.result }}"
        echo "Integration Status: ${{ needs.integration.result }}"
        echo "Security Status: ${{ needs.security.result }}"
        echo "Docs Status: ${{ needs.docs.result }}"

        if [[ "${{ needs.test.result }}" == "success" && "${{ needs.integration.result }}" == "success" ]]; then
          echo "✅ Core functionality verified across all platforms!"
        else
          echo "❌ Some tests failed - check the logs above"
        fi